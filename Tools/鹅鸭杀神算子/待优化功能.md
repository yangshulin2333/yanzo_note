- [x] 已完成任务
- [ ] 未完成任务
- [ ] 待办事项

- [ ] 自动获取游戏ID


我们把整个流程拆解开，按照你说的思路，分为 **“大厅阶段”** 和 **“游戏内阶段”** 来讨论方案。

---

### 一、 大厅阶段：你提到的“固定坐标找色”绝对是首选

在大厅（Lobby）或者匹配界面，UI 是**绝对静止**的。这里的确不需要 AI 大模型介入。

**方案推荐：传统脚本逻辑（找色/OCR）**

- **为什么要用它？**
    
    - **速度快**：毫秒级响应。
        
    - **零成本**：不消耗 API 额度。
        
    - **逻辑简单**：只要判断某个坐标是不是“绿色”，就知道是不是“准备好了”；只要 OCR 扫描一下玩家列表区域，就知道有谁进来了。
        
- **它负责什么？**
    
    1. **检测开局**：比如检测屏幕右下角“开始游戏”按钮从灰色变亮，或者屏幕中间出现倒计时。一旦检测到，立刻通知后台切换状态到 `InGame`。
        
    2. **记录玩家 ID**：在备战界面，玩家列表是固定的列表。直接用本地 OCR（比如 Windows 自带的 OCR 库）把这 16 个人的名字扫下来，存入内存。
        
    3. **识别板子/地图**：同样是固定位置的文字，OCR 扫一下就知道是“地下室”还是“庄园”。
        

**结论**：你的思路完全正确。大厅阶段就用“找色+本地OCR”，把**基础信息（玩家名单、地图）** 喂给程序，作为初始数据。

---

### 二、 游戏内阶段：真正的难点与方案

进入游戏后，情况完全变了：

1. **动态画面**：你的角色在走，背景在动，没法用“固定坐标”来判断尸体在哪里。
    
2. **光照变化**：迷雾、关灯技能会导致找色失效。
    

这时候我们需要 **“实时扫描”** 和 **“辅助记忆”**，我有以下三个层级的方案，看你觉得哪个合适：

#### 方案 A：轻量级 OCR 实时扫描（推荐：性价比最高）

这个方案的核心是：**不把图片发给 AI，只把图片里的文字（名字）提取出来。**

- **原理**：
    
    - 《鹅鸭杀》里，只要玩家出现在你的视野里，头顶一定有**名字 ID**。
        
    - 我们可以在本地运行一个轻量级的 OCR（文字识别），每秒扫 2-3 次屏幕。
        
- **能做到什么？**
    
    - **实时记录轨迹**：程序会不断记录：“10:01:05 看到【张三】”、“10:01:08 看到【李四】”。
        
    - **甚至判断距离**：名字越大，离你越近；名字越小，离你越远。
        
- **优点**：完全不消耗 token，实时性极高。
    
- **配合 AI**：当发现尸体或者开会时，程序把这份**“目击记录”**打包发给 DeepSeek：“我刚才一分钟内见过了张三和李四，现在张三死了。” —— AI 瞬间就能推断李四有嫌疑。
    

#### 方案 B：关键事件触发截图（精准打击）

不要一直截图分析，而是通过“声音”或“特定UI”来触发 AI 视觉。

- **原理**：
    
    - **听觉触发**：当 `AudioService` 监听到有人喊“死人了”、“报警了”，立刻截一张图发给 GroqVision，分析现场。
        
    - **UI 变化触发**：利用传统的找色，检测右侧的“报警/鸡腿”按钮是否亮起。一旦亮起，说明周围有尸体，立刻截图分析尸体是谁（或者尸体穿什么衣服）。
        
- **优点**：好钢用在刀刃上，只在出事的时候用 AI 眼睛。
    

#### 方案 C：YOLO 目标检测（最硬核，但开发成本高）

这就是类似特斯拉自动驾驶的原理。训练一个专门识别“鹅”、“鸭子”、“尸体”的小模型。

- **原理**：
    
    - 不需要联网，在本地跑一个 YOLO 模型，实时框出屏幕上的物体。
        
- **缺点**：你需要成千上万张《鹅鸭杀》的截图来训练模型，工程量巨大，不建议个人开发者现在就搞这个。
    

---

### 三、 总结：最适合你的混合架构方案

既然我们要“实时辅助”，我建议采用 **“大厅找色 + 局内 OCR + 关键时刻 AI”** 的混合打法：

1. **大厅（准备阶段）**：
    
    - **手段**：Windows API 取色 + 本地 OCR。
        
    - **目的**：获取本局**玩家名单**、**地图**、**职业配置**。程序在后台建立一个“档案库”。
        
2. **对局中（巡逻阶段）**：
    
    - **手段**：**全屏本地 OCR 扫描**（每 0.5 秒一次）。
        
    - **逻辑**：
        
        - 程序在后台默默记录：“我在走廊看到了 A”，“我在饭堂看到了 B”。
            
        - **不需要 AI 介入**，纯本地记录，不卡顿。
            
3. **对局中（突发事件）**：
    
    - **手段**：**Groq Vision + DeepSeek**。
        
    - **触发条件**：
        
        - 听到“死人了”或者手动按了报警。
            
        - 或者你手动按快捷键（比如 F1）。
            
    - **逻辑**：程序把刚才 OCR 记录的**“目击者清单”**（比如：刚才 A 和 B 在一起）+ **当前的尸体截图**，一起打包发给 DeepSeek。
        

**效果演示：**

> 此时开饭（投票）。
> 
> 你问 AI：“刚才谁嫌疑最大？”
> 
> **AI (拥有记忆)**：“根据我的记录，尸体发现前 10 秒，你的视野里【王五】和死者【赵六】同时走向了锅炉房，之后只有【王五】出来了。建议全票打飞【王五】。”

你觉得这个 **“本地 OCR 记录数据 + AI 分析决策”** 的方案，是不是更符合你想要的“实时辅助”？如果是，我们再谈具体怎么实现“本地 OCR”。






GooseMind/
│
├── core/                # 【核心逻辑层】存放高内聚的功能模块
│   ├── __init__.py
│   ├── capture.py       # 专门负责：找窗口、截屏（高频采集引擎）
│   └── vision.py        # 专门负责：图像处理、OCR、特征比对
│
├── data/                # 【数据层】存放配置和静态资源
│   ├── config.py        # 存放 API Key、窗口标题、坐标等配置
│   └── assets/          # 存放用于比对的图标素材（如“组局”按钮截图）
│
├── logs/                # 【日志层】存放对局记录
│
└── main.py              # 【启动入口】低耦合的指挥官，负责调用各模块


- [ ] 获取句柄后遮挡问题待处理